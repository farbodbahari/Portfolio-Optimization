{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .container{width:96% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import datetime as dt\n",
    "pd.options.display.min_rows=1000\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.min_rows', 1000)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "\n",
    "import pypfopt\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt import black_litterman, risk_models\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt import EfficientFrontier\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style> .container{width:96% !important;}</style>\"))\n",
    "pd.options.display.max_rows=150\n",
    "pd.options.display.max_columns=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cash</th>\n",
       "      <th>BondGov</th>\n",
       "      <th>BondIG</th>\n",
       "      <th>BondHY</th>\n",
       "      <th>EquityUS</th>\n",
       "      <th>EquityEAFE</th>\n",
       "      <th>EquityEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988-01-31</th>\n",
       "      <td>0.00473</td>\n",
       "      <td>0.04684</td>\n",
       "      <td>0.03830</td>\n",
       "      <td>0.02909</td>\n",
       "      <td>0.03960</td>\n",
       "      <td>0.01674</td>\n",
       "      <td>0.09419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-02-29</th>\n",
       "      <td>0.00471</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>0.01680</td>\n",
       "      <td>0.02756</td>\n",
       "      <td>0.03873</td>\n",
       "      <td>0.06558</td>\n",
       "      <td>0.00240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-03-31</th>\n",
       "      <td>0.00476</td>\n",
       "      <td>-0.02319</td>\n",
       "      <td>-0.01139</td>\n",
       "      <td>-0.00192</td>\n",
       "      <td>-0.03651</td>\n",
       "      <td>0.06036</td>\n",
       "      <td>0.09993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Cash  BondGov   BondIG   BondHY  EquityUS  EquityEAFE  EquityEM\n",
       "Date                                                                         \n",
       "1988-01-31 0.00473  0.04684  0.03830  0.02909   0.03960     0.01674   0.09419\n",
       "1988-02-29 0.00471  0.01222  0.01680  0.02756   0.03873     0.06558   0.00240\n",
       "1988-03-31 0.00476 -0.02319 -0.01139 -0.00192  -0.03651     0.06036   0.09993"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load data\n",
    "df_stock_return= pd.read_excel(\"Index Monthly Time Series.xlsx\")\n",
    "df_stock_return=df_stock_return.set_index(\"Date\")\n",
    "df_stock_return.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stock_return_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2a8084203fa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### Cumulative Return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mdf_stock_return_data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cumulative Return\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_stock_return_data' is not defined"
     ]
    }
   ],
   "source": [
    "#### Cumulative Return\n",
    "(df_stock_return_data+1).cumprod().plot(figsize=(10,5));\n",
    "plt.title(\"Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_CVaR(data_series,\n",
    "             significance_level):\n",
    "    \"\"\"Return VaR and CVaR and Tail Loss respectivley at the specifeid significance_level \n",
    "    1. VaR Based on Normal_Dist and sample mean and std \n",
    "    2. VaR Based on specified significance level quntile\n",
    "    3. CVaR Based on avergae extreme values below VaR (VaR based on specified quantile)\n",
    "    4. CVaR Based on Normal_Dist, sample mean and std, and VaR based on Quantile\n",
    "    \n",
    "    CVaR = (1/significance_level) * tail_loss\n",
    "    \"\"\"\n",
    "    import scipy.stats\n",
    "    import numpy as np\n",
    "    mean=np.mean(data_series)\n",
    "    std= np.std(data_series)\n",
    "    #### Normal Dist VaR CVaR\n",
    "    VaR_Normal_Dist=scipy.stats.norm.ppf(\n",
    "        significance_level,\n",
    "        loc = mean,\n",
    "        scale = std)\n",
    "    return_in_tail_based_on_VaR_Normal_Dist=data_series[data_series<VaR_Normal_Dist]\n",
    "    \n",
    "    tail_loss_Normal_dist = scipy.stats.norm.expect(lambda x: x,\n",
    "                            loc = mean,\n",
    "                            scale = std,\n",
    "                            ub = VaR_Normal_Dist)\n",
    "    CVaR_Normal = (1/significance_level) * tail_loss_Normal_dist\n",
    "    \n",
    "    \n",
    "    #### T Dist Var CVaR\n",
    "    t_dist_params = scipy.stats.t.fit(data_series)\n",
    "    \n",
    "    VaR_t_dist = scipy.stats.t.ppf(significance_level,*t_dist_params)\n",
    "    tail_loss_t= scipy.stats.t.expect( lambda y: y, args = (t_dist_params[0],),\n",
    "                                      loc = t_dist_params[1],\n",
    "                                      scale = t_dist_params[2],\n",
    "                                      ub = VaR_t_dist)\n",
    "    \n",
    "    CVaR_T = (1/significance_level) * tail_loss_t\n",
    "    \n",
    "    \n",
    "    ### Data Driven VaR CVaR\n",
    "    VaR_Quantile = np.quantile(data_series,significance_level)\n",
    "    return_in_tail_based_on_VaR_Quantile=data_series[data_series<VaR_Quantile]\n",
    "    CVaR_Quantile=np.mean(return_in_tail_based_on_VaR_Quantile)    \n",
    "    \n",
    "    \n",
    "    #### VaR and CVaR GEV \n",
    "    genextreme_params= scipy.stats.genextreme.fit(-1*data_series)\n",
    "    VaR_GEV = (scipy.stats.genextreme.ppf((1-significance_level), *genextreme_params))\n",
    "    \n",
    "    tail_loss_GEV = scipy.stats.genextreme.expect(lambda x: x, \n",
    "           args=(genextreme_params[0],),\n",
    "                                                   loc = genextreme_params[1], \n",
    "                                              scale = genextreme_params[2],\n",
    "                                              lb = VaR_GEV)\n",
    "    CVaR_GEV = ((1/(significance_level)) * tail_loss_GEV)\n",
    "    \n",
    "    \n",
    "    return ({\"VaR_Normal_Dist\":VaR_Normal_Dist,\n",
    "             \"VaR_GEV\":VaR_GEV*-1,\n",
    "             \"VaR_t_dist\":VaR_t_dist,\n",
    "             \"VaR_Quantile\":VaR_Quantile,\n",
    "             \n",
    "            \"CVaR_Normal\":CVaR_Normal,\n",
    "            \"CVaR_GEV\":CVaR_GEV*-1,\n",
    "             \"CVaR_T\":CVaR_T,\n",
    "             \"CVaR_Quantile\":CVaR_Quantile\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_CVaR_Quantile(data_series,\n",
    "                      significance_level):\n",
    "    \"\"\"Returnn Value at Risk (VaR) and Conditional Value at Risk (CVaR) at significance_level\"\"\"\n",
    "    import scipy.stats\n",
    "    import numpy as np\n",
    "    mean=np.mean(data_series)\n",
    "    std= np.std(data_series)\n",
    "    ### Data Driven VaR CVaR\n",
    "    VaR_Quantile = np.quantile(data_series,significance_level)\n",
    "    return_in_tail_based_on_VaR_Quantile=data_series[data_series<VaR_Quantile]\n",
    "    CVaR_Quantile=(np.mean(return_in_tail_based_on_VaR_Quantile))\n",
    "    return(VaR_Quantile,CVaR_Quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return_series_computer(df_return_data,\n",
    "                                     df_weights,\n",
    "                                     list_of_assets_columns):\n",
    "    \"\"\"This function is to create return series of a portfolo taking weights of assest and return of assest\"\"\"\n",
    "    df_portfolios_return=df_return_data[list_of_assets_columns].dot(\n",
    "        df_weights[list_of_assets_columns].T)\n",
    "    #df_portfolios_return.reset_index(inplace=True)\n",
    "    df_portfolios_return=pd.melt(df_portfolios_return,var_name=\"portfolio\",value_name=\"return\")\n",
    "    return(df_portfolios_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_stats_computer(portfolio_return_series,\n",
    "                            frequency=252):\n",
    "    \"\"\"This Function is to computer Annualized Return and Volatility of a series given the series and its frequency\n",
    "    VaR and CVaR are at 5%\"\"\"\n",
    "    portfolio_return_series=pd.Series(portfolio_return_series)\n",
    "    \n",
    "    portfolio_annualized_compouded_return =(1 + portfolio_return_series).prod() ** (frequency / portfolio_return_series.count()) - 1\n",
    "    portfolio_annualized_simple_return = portfolio_return_series.mean()*frequency\n",
    "    portfolio_annualized_volatility= portfolio_return_series.std()*np.sqrt(frequency)\n",
    "    portfolio_VaR_05, portfolio_CVaR_05 = VaR_CVaR_Quantile(portfolio_return_series,0.05)\n",
    "    portfolio_sharp_ratio = portfolio_annualized_compouded_return/portfolio_annualized_volatility\n",
    "    \n",
    "    portfolio_kurtosis= scipy.stats.kurtosis(portfolio_return_series)+3\n",
    "    portfolio_skewness= scipy.stats.skew(portfolio_return_series)\n",
    "    \n",
    "    dict_stats={\"Annualized Simple Return\":[portfolio_annualized_simple_return],\n",
    "                \"Annualized Compunded Return\":[portfolio_annualized_compouded_return],\n",
    "                \"Annualized Volatility\":[portfolio_annualized_volatility],\n",
    "                \"Sharp Ratio\":[portfolio_sharp_ratio],\n",
    "                \"Kurtosis\":[portfolio_kurtosis],\n",
    "                \"Skewness\":[portfolio_skewness],\n",
    "                \"VaR 5\":[portfolio_VaR_05],\n",
    "                \"CVaR 5\":[portfolio_CVaR_05]}\n",
    "    return pd.DataFrame(dict_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_performance_computer(df_return_data,\n",
    "                                   df_weights,\n",
    "                                   list_of_assets_columns,\n",
    "                                   frequency=252):\n",
    "    \"\"\"This function utlize portfolio_return_series_computer and portfolio_stats_computer to compute \n",
    "    Annualized Simple Return and Annualized Volatility of portfolios. The inputs are:\n",
    "    df_return_data: return of the assets and \n",
    "    df_weights:weights of assets in each portfolio.\n",
    "    frequency: frequency of the return data\"\"\"\n",
    "    df_portfolios_return_series = portfolio_return_series_computer(df_return_data,\n",
    "                                                                   df_weights,\n",
    "                                                                   list_of_assets_columns)\n",
    "    \n",
    "    df_portfolios_return_series_stats=df_portfolios_return_series.groupby(\n",
    "        \"portfolio\")[\"return\"].apply(lambda x: portfolio_stats_computer(x,frequency=frequency))\n",
    "    list_of_stats=df_portfolios_return_series_stats.columns.to_list()\n",
    "    df_portfolios_return_series_stats.reset_index(inplace=True)\n",
    "    df_portfolios_return_series_stats=pd.concat([df_weights,\n",
    "                                                 df_portfolios_return_series_stats],\n",
    "                                                ignore_index=False,axis=1)\n",
    "    df_portfolios_return_series_stats.sort_values([\"Annualized Volatility\"],inplace=True)\n",
    "    df_portfolios_return_series_stats.reset_index(drop=True,inplace=True)\n",
    "    return df_portfolios_return_series_stats.drop([\"portfolio\",\"level_1\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_return_data_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_portfolio_instance(df_return_data,\n",
    "                                 frequency=252,\n",
    "                                 covarirance_method=\"ledoit_wolf\",\n",
    "                                 compound_return=False,\n",
    "                                 weight_bounds=(0,1)):\n",
    "    \"\"\"df_return_data: asset prices or return\n",
    "    frequency: frequency of your data, by default is 252 (Daily Return)\n",
    "    compound_return: comound the return or simple return. Set it True for Compounding and False for simple return\n",
    "    risk_aversion: utility,\n",
    "    covarirance_method: sample_cov,semicovariance, exp_cov, min_cov_determinant, ledoit_wolf,\n",
    "    ledoit_wolf_constant_variance, ledoit_wolf_single_factor, ledoit_wolf_constant_correlation,oracle_approximating\n",
    "    target_risk: amount of annualized volatility. If not specified, it optimzes for the lowest CVaR portfolio\"\"\"\n",
    "    ### Get Annualized Mean and Covairance\n",
    "    annualized_return = mean_historical_return(df_return_data,\n",
    "                                               returns_data=True,\n",
    "                                               frequency=frequency,\n",
    "                                               compounding=compound_return)\n",
    "    \n",
    "    annualized_cov= pypfopt.risk_models.risk_matrix(df_return_data,\n",
    "                                                    returns_data=True,\n",
    "                                                    method=covarirance_method,\n",
    "                                                    frequency=frequency)\n",
    "    ef= EfficientFrontier(annualized_return,\n",
    "                          annualized_cov,\n",
    "                          weight_bounds=weight_bounds)\n",
    "    return(ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_calculator(df_return_data,\n",
    "                                  frequency=252,\n",
    "                                  covarirance_method=\"ledoit_wolf\",\n",
    "                                  compound_return=False,\n",
    "                                  portfolio_performance=False,\n",
    "                                  weight_bounds=(0,1),\n",
    "                                  volatility_point=1000):\n",
    "    list_of_assets=df_return_data.columns.tolist()\n",
    "    ef_for_min_vol = efficient_portfolio_instance(df_return_data,\n",
    "                                                  frequency=frequency,\n",
    "                                                  covarirance_method=covarirance_method,\n",
    "                                                  compound_return=compound_return,\n",
    "                                                  weight_bounds=weight_bounds)\n",
    "    ef_for_min_vol.min_volatility()\n",
    "    min_possible_annualized_volatility = ef_for_min_vol.portfolio_performance()[1] + 0.001\n",
    "    max_possible_annualzied_volatility = (df_return_data.std()*np.sqrt(frequency)).max() + 0.001\n",
    "    delta = (max_possible_annualzied_volatility - min_possible_annualized_volatility)\n",
    "    ### Generate enough number of evenly spaced point of volatility\n",
    "    range_of_annualzied_volatility=np.linspace(min_possible_annualized_volatility,\n",
    "                                               max_possible_annualzied_volatility,\n",
    "                                               num=int(delta*volatility_point))\n",
    "    list_of_df_weights=[]\n",
    "    for target_risk in range_of_annualzied_volatility:\n",
    "        try:\n",
    "            ef = efficient_portfolio_instance(df_return_data,\n",
    "                                             frequency=frequency,\n",
    "                                             covarirance_method=covarirance_method,\n",
    "                                             compound_return=compound_return,\n",
    "                                             weight_bounds=weight_bounds)\n",
    "            ef.efficient_risk(target_risk)\n",
    "            ### Get The Weights of asset in portfolio\n",
    "            df_weights = pd.DataFrame([ef.weights],\n",
    "                                      columns=list_of_assets)\n",
    "            df_weights[\"target_risk\"]=target_risk\n",
    "            list_of_df_weights.append(df_weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ### Concatinate all the Portfolios with target risk\n",
    "    df_efficient_frontier=pd.concat(list_of_df_weights,\n",
    "                                    ignore_index=True)\n",
    "    \n",
    "    ### There are replicated portfolios, get rid of them\n",
    "    df_efficient_frontier=df_efficient_frontier.drop_duplicates(list_of_assets).reset_index(drop=True)\n",
    "    ### Computer Portfolio Performance\n",
    "    if portfolio_performance:\n",
    "        df_efficient_frontier=portfolio_performance_computer(df_return_data,\n",
    "                                                             df_efficient_frontier,\n",
    "                                                             list_of_assets,\n",
    "                                                             frequency=frequency)\n",
    "    return(df_efficient_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shaker(df_return_data,\n",
    "                shake_magnitude=.05,\n",
    "                multivariate_normal=True):\n",
    "    \"\"\"Add some random white noise with mean zeror some magnitude of the covariance matrix\n",
    "    multivariate_normal: Add some random white noise with mean zeror magnitude of the standard deviation of the underlying asset. \"\"\"\n",
    "    if multivariate_normal:\n",
    "        random_noise= np.random.multivariate_normal(df_return_data.mean()*0,\n",
    "                                                    df_return_data.cov(),\n",
    "                                                    len(df_return_data))\n",
    "        df_return_data_with_random_noise= df_return_data + (random_noise*shake_magnitude)\n",
    "    \n",
    "    if multivariate_normal == False:\n",
    "        number_of_observation=df_return_data.shape[0]\n",
    "        number_of_asset=df_return_data.shape[1]\n",
    "        data_std=df_return_data.std()\n",
    "        random_noise= np.random.normal(loc=0,\n",
    "                                      scale=data_std,\n",
    "                                      size=(number_of_observation,number_of_asset))\n",
    "        df_return_data_with_random_noise= df_return_data + (random_noise*shake_magnitude)\n",
    "    return(df_return_data_with_random_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_return_simulator(df_return_data,\n",
    "                                         number_of_return_series_to_be_simulated):\n",
    "    \"\"\"multivariate_normal Return simulator\"\"\"\n",
    "    list_of_assets = df_return_data.columns.to_list()\n",
    "    ### Simulate the returns\n",
    "    simulated_returns = np.random.multivariate_normal(df_return_data.mean(),\n",
    "                                                      df_return_data.cov(),\n",
    "                                                      number_of_return_series_to_be_simulated)\n",
    "    ### Put those simulated returns into DataFrame\n",
    "    df_simulated_returns = pd.DataFrame(simulated_returns,\n",
    "                                        columns=list_of_assets)\n",
    "    return(df_simulated_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_simulator_multivariate_normal(df_return_data,\n",
    "                                                    number_of_simulation=10,\n",
    "                                                    frequency=252,\n",
    "                                                    compound_return=False,\n",
    "                                                    covarirance_method=\"ledoit_wolf\",\n",
    "                                                    weight_bounds=(0, 1),\n",
    "                                                    volatility_point=1000):\n",
    "    \"\"\" Simulate Multi variate Normal Distribution return from actual data. Then construct number_of_simulation efficinet \n",
    "    frontier.\n",
    "    df_return_data: returns of assets\n",
    "    number_of_simulation: number of simulation to run\n",
    "    frequency: frequency of data, daily=252, monthly =12 etc.\n",
    "    compound_return: compouding return or simple return\"\"\"\n",
    "    list_of_assets=df_return_data.columns.tolist()\n",
    "    ### Consurtct Efficient Frontiter with simulated Return and Covriance matrix\n",
    "    list_of_simulated_portfolios=[]\n",
    "    for i in range(number_of_simulation):\n",
    "        df_data_return_simulated= multivariate_normal_return_simulator(df_return_data,\n",
    "                                                                       len(df_return_data))\n",
    "        ### Don't calcualte portfolio perfomrnace the with efficient_frontier_calculator\n",
    "        ### Because the returns are simulated return, not actual\n",
    "        df_weights = efficient_frontier_calculator(df_data_return_simulated,\n",
    "                                                   frequency=frequency,\n",
    "                                                   compound_return=compound_return,\n",
    "                                                   covarirance_method=covarirance_method,\n",
    "                                                   portfolio_performance=False,\n",
    "                                                   weight_bounds=weight_bounds,\n",
    "                                                   volatility_point=volatility_point)\n",
    "        df_weights[\"Simulation\"]=i\n",
    "        list_of_simulated_portfolios.append(df_weights)\n",
    "    ### Concatinate all the simulated returns into 1 dataframe\n",
    "    df_simulated_efficient_frontier = pd.concat(list_of_simulated_portfolios,\n",
    "                                                ignore_index=True)\n",
    "    \n",
    "    ### Compute the return and volatility of each portfolio based on actual return data\n",
    "    df_simulated_efficient_frontier= portfolio_performance_computer(df_return_data,\n",
    "                                                                    df_simulated_efficient_frontier,\n",
    "                                                                    list_of_assets,\n",
    "                                                                    frequency)\n",
    "    \n",
    "    return(df_simulated_efficient_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_simulator_shaking_returns(df_return_data,\n",
    "                                                 list_of_shaking_magnitude=[10,15,20,25,30,35,40],\n",
    "                                                 frequency=252,\n",
    "                                                 compound_return=False,\n",
    "                                                 covarirance_method=\"ledoit_wolf\",\n",
    "                                                 multivariate_normal=True,\n",
    "                                                 volatility_point=1000,\n",
    "                                                 weight_bounds=(0,1)):\n",
    "    list_of_assets=df_return_data.columns.tolist()\n",
    "    \"\"\" Inject white noise with mean 0 and std*%shaking_magnitude into the resturn to consturct \n",
    "    multiple efficient frontier\n",
    "    df_return_data: Retuns of asstes\n",
    "    list_of_shaking_magnitude: list of magnitude to shake the returns, based on percentage of standard deviation of the\n",
    "    underlying asset  20% 10% etc.\n",
    "    frequency: data frequency. Default 252 daily. 12 monthly etc.\n",
    "    \"\"\"\n",
    "    list_of_simulated_portfolios=[]\n",
    "    for shake in list_of_shaking_magnitude:\n",
    "        shake=shake/100\n",
    "        df_data_return_simulated = data_shaker(df_return_data,\n",
    "                                               shake_magnitude=shake,\n",
    "                                               multivariate_normal=multivariate_normal)\n",
    "        ### Don't calcualte portfolio perfomrnace the with efficient_frontier_calculator\n",
    "        ### Because the returns are simulated return, not actual\n",
    "        df_weights = efficient_frontier_calculator(df_data_return_simulated,\n",
    "                                                   frequency=frequency,\n",
    "                                                   compound_return=compound_return,\n",
    "                                                   covarirance_method=covarirance_method,\n",
    "                                                   portfolio_performance=False,\n",
    "                                                   weight_bounds=weight_bounds,\n",
    "                                                   volatility_point=volatility_point)\n",
    "        df_weights[\"shake\"] = shake\n",
    "        list_of_simulated_portfolios.append(df_weights)\n",
    "    \n",
    "    ### Concatinate all the portfolios\n",
    "    df_simulated_efficient_frontier=pd.concat(list_of_simulated_portfolios,\n",
    "                                              ignore_index=True)\n",
    "    ### Compute Portfolios performances\n",
    "    df_simulated_efficient_frontier=portfolio_performance_computer(df_return_data,\n",
    "                                                                   df_simulated_efficient_frontier,\n",
    "                                                                   list_of_assets,\n",
    "                                                                   frequency)\n",
    "    return (df_simulated_efficient_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bin_qty_comuter(values,\n",
    "                         spacing):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \"\"\"Determine how many bins are needed for eqaully spaced bins with the spacing specified\"\"\"\n",
    "    values_series=pd.Series(values)\n",
    "    values_series_min=values_series.min()\n",
    "    values_series_max=values_series.max()\n",
    "    values_series_evenly_spaced=np.arange(values_series_min-spacing,\n",
    "                                          values_series_max+spacing,\n",
    "                                          spacing)\n",
    "    number_of_bins_for_splititng_volatilies= len(values_series_evenly_spaced)\n",
    "    return(number_of_bins_for_splititng_volatilies)\n",
    "\n",
    "def portfolio_resampler(volatility_series,\n",
    "                        volatility_spacing):\n",
    "    volatility_series=pd.Series(volatility_series)\n",
    "    number_of_bins = data_bin_qty_comuter(values = volatility_series,\n",
    "                                          spacing =volatility_spacing)\n",
    "    return (pd.cut(volatility_series,number_of_bins))\n",
    "\n",
    "def efficient_frontier_resampler(df_efficient_frontiers,\n",
    "                                 df_return_data,\n",
    "                                 volatility_spacing=.005,\n",
    "                                 frequency=252):\n",
    "    list_of_assets = df_return_data.columns.tolist()\n",
    "    df_weights_resampled = df_efficient_frontiers.copy()\n",
    "    ### Resmaple the portfolios based on volatility bins\n",
    "    df_weights_resampled[\"Volatility Bins\"]= portfolio_resampler(df_weights_resampled[\"Annualized Volatility\"],\n",
    "                                                                 volatility_spacing)\n",
    "    df_weights_resampled[\"Volatility Bins\"]=df_weights_resampled[\"Volatility Bins\"].astype(str)\n",
    "    ### Get the Average weight of each asset in resampled portfolios from each bin\n",
    "    df_weights_resampled = df_weights_resampled.groupby([\"Volatility Bins\"])[list_of_assets].mean().reset_index().dropna()\n",
    "    ### Compute the performance of the resmapled portfolios\n",
    "    df_weights_resampled=portfolio_performance_computer(df_return_data,\n",
    "                                                        df_weights_resampled,\n",
    "                                                        list_of_assets,\n",
    "                                                        frequency)\n",
    "    return(df_weights_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_portfolio_CVaR_calculator(df_return_data,\n",
    "                                        CVaR_Significance=0.05,\n",
    "                                        target_risk=None,\n",
    "                                        frequency=252,\n",
    "                                        compound_return=False,\n",
    "                                        weight_bounds=(0,1),\n",
    "                                        covarirance_method=\"sample_cov\",\n",
    "                                        solver=\"SLSQP\",\n",
    "                                        portfolio_performance=False):\n",
    "    \"\"\"covarirance_method: sample_cov,semicovariance, exp_cov, min_cov_determinant, ledoit_wolf,\n",
    "    ledoit_wolf_constant_variance, ledoit_wolf_single_factor, ledoit_wolf_constant_correlation,oracle_approximating\n",
    "    target_risk: amount of annualized volatility. If not specified, it optimzes for the lowest CVaR portfolio\"\"\"\n",
    "    list_of_assets= df_return_data.columns.tolist()\n",
    "    ### CVAR Faunction to be minimiazedd\n",
    "    def CVaR_Quantile_Method(w,\n",
    "                             return_series,\n",
    "                             significance_level):\n",
    "        import scipy.stats\n",
    "        import numpy as np\n",
    "        data_series = return_series.dot(w)\n",
    "        mean = np.mean(data_series)\n",
    "        std = np.std(data_series)\n",
    "        ### Data Driven VaR CVaR\n",
    "        VaR_Quantile = np.quantile(data_series,significance_level)\n",
    "        return_in_tail_based_on_VaR_Quantile=data_series[data_series<VaR_Quantile]\n",
    "        CVaR_Quantile=(np.mean(return_in_tail_based_on_VaR_Quantile))*(-1)\n",
    "        return(CVaR_Quantile)\n",
    "    \n",
    "    ef = efficient_portfolio_instance(df_return_data,\n",
    "                                      frequency=frequency,\n",
    "                                      covarirance_method=covarirance_method,\n",
    "                                      compound_return=compound_return,\n",
    "                                      weight_bounds=weight_bounds)\n",
    "    \n",
    "    ### Minimize the CVaR function subject to no constrian\n",
    "    if target_risk==None:\n",
    "        ef.nonconvex_objective(CVaR_Quantile_Method,\n",
    "                               solver=solver,\n",
    "                               weights_sum_to_one=True,\n",
    "                               objective_args=(df_return_data.values,\n",
    "                                               CVaR_Significance))\n",
    "    else:\n",
    "        # Portfolio vol less than target vol\n",
    "        risk_constr = {\"type\": \"eq\",\n",
    "                       \"fun\": lambda w: target_risk ** 2 - np.dot(w.T, np.dot(ef.cov_matrix, w))}\n",
    "        constraints = [risk_constr]\n",
    "        ### Minimize the CVaR function with subject to Volatility constrian\n",
    "        ef.nonconvex_objective(CVaR_Quantile_Method,\n",
    "                               solver=solver,\n",
    "                               constraints=constraints,\n",
    "                               weights_sum_to_one=True,\n",
    "                               objective_args=(df_return_data.values,\n",
    "                                               CVaR_Significance))    \n",
    "    ### Get The Weights of asset in portfolio\n",
    "    #### Store the outputs into the dataframe \n",
    "    df_weights =  pd.DataFrame([ef.weights],\n",
    "                                      columns=list_of_assets)\n",
    "    df_weights[\"CVaR Significance\"]=CVaR_Significance\n",
    "    \n",
    "    ### Computer Portfolio Performance\n",
    "    if portfolio_performance:\n",
    "        df_weights=portfolio_performance_computer(df_return_data,\n",
    "                                                  df_weights,\n",
    "                                                  list_of_assets,\n",
    "                                                  frequency=frequency)\n",
    "    return(df_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_CVaR_calculator(df_return_data,\n",
    "                                       CVaR_Significance=0.05,\n",
    "                                       frequency=252,\n",
    "                                       compound_return=False,\n",
    "                                       weight_bounds=(0,1),\n",
    "                                       covarirance_method=\"sample_cov\",\n",
    "                                       solver=\"SLSQP\",\n",
    "                                       portfolio_performance=False,\n",
    "                                       volatility_point=1000):\n",
    "    \"\"\"covarirance_method: sample_cov,semicovariance, exp_cov, min_cov_determinant, ledoit_wolf,\n",
    "    ledoit_wolf_constant_variance, ledoit_wolf_single_factor, ledoit_wolf_constant_correlation,oracle_approximating\n",
    "    volatility_points: Number of evenly spaced volatility portfolio to be created if you need more precise volatility CVaR portfolio,\n",
    "    increase it. Will be computationaly more expensive \"\"\"\n",
    "    list_of_assets= df_return_data.columns.tolist()\n",
    "    ### CVAR Faunction to be minimiazedd\n",
    "    def CVaR_Quantile_Method(w,\n",
    "                             return_series,\n",
    "                             significance_level):\n",
    "        import scipy.stats\n",
    "        import numpy as np\n",
    "        data_series = return_series.dot(w)\n",
    "        mean = np.mean(data_series)\n",
    "        std = np.std(data_series)\n",
    "        ### Data Driven VaR CVaR\n",
    "        VaR_Quantile = np.quantile(data_series,significance_level)\n",
    "        return_in_tail_based_on_VaR_Quantile=data_series[data_series<VaR_Quantile]\n",
    "        CVaR_Quantile=(np.mean(return_in_tail_based_on_VaR_Quantile))*(-1)\n",
    "        return(CVaR_Quantile)\n",
    "\n",
    "    \n",
    "    ef = efficient_portfolio_instance(df_return_data,\n",
    "                                      frequency=frequency,\n",
    "                                      covarirance_method=covarirance_method,\n",
    "                                      compound_return=compound_return,\n",
    "                                      weight_bounds=weight_bounds)\n",
    "    \n",
    "    ### Compute Min Volatility Portfolio\n",
    "    ef.min_volatility()\n",
    "    min_possible_annualized_volatility = ef.portfolio_performance()[1] + 0.001\n",
    "    max_possible_annualzied_volatility = (df_return_data.std()*np.sqrt(frequency)).max() +  0.001\n",
    "    delta = (max_possible_annualzied_volatility - min_possible_annualized_volatility)\n",
    "    \n",
    "    \n",
    "    ### Generate enough number of evenly spaced point of volatility\n",
    "    range_of_annualzied_volatility=np.linspace(min_possible_annualized_volatility,\n",
    "                                               max_possible_annualzied_volatility,\n",
    "                                               num=int(delta*volatility_point))\n",
    "    \n",
    "    ef = efficient_portfolio_instance(df_return_data,\n",
    "                                      frequency=frequency,\n",
    "                                      covarirance_method=covarirance_method,\n",
    "                                      compound_return=compound_return,\n",
    "                                      weight_bounds=weight_bounds)\n",
    "    \n",
    "    #### Portfolio vol less than target vol\n",
    "    risk_constr = {\"type\": \"eq\",\n",
    "                   \"fun\": lambda w: target_risk ** 2 - np.dot(w.T, np.dot(ef.cov_matrix, w))}\n",
    "    constraints = [risk_constr]\n",
    "    list_of_df_weights=[]\n",
    "    ### Minimize the CVaR function with subject to Volatility constrian\n",
    "    for target_risk in range_of_annualzied_volatility:\n",
    "        try:\n",
    "            ef.nonconvex_objective(CVaR_Quantile_Method,\n",
    "                                   solver=solver,\n",
    "                                   weights_sum_to_one=True,\n",
    "                                   constraints=constraints,\n",
    "                                   objective_args=(df_return_data.values,\n",
    "                                                   CVaR_Significance))\n",
    "            ### Get The Weights of asset in portfolio\n",
    "            df_weights =  pd.DataFrame([ef.weights],\n",
    "                                      columns=list_of_assets)\n",
    "            #### Store the outputs into the dataframe \n",
    "            df_weights[\"CVaR Significance\"]=CVaR_Significance\n",
    "            list_of_df_weights.append(df_weights)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    ### Concatinate all the CVaR Portfolios\n",
    "    df_efficient_frontier=pd.concat(list_of_df_weights,\n",
    "                                    ignore_index=True)\n",
    "  \n",
    "    ### Computer Portfolio Performance\n",
    "    if portfolio_performance:\n",
    "        df_efficient_frontier=portfolio_performance_computer(df_return_data,\n",
    "                                                             df_efficient_frontier,\n",
    "                                                             list_of_assets,\n",
    "                                                             frequency=frequency)\n",
    "    return(df_efficient_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_CVaR_simulator_shaking_returns(df_return_data,\n",
    "                                                     list_of_shaking_magnitude=[30,35,40,45,50],\n",
    "                                                      multivariate_normal=True,\n",
    "                                                     CVaR_Significance=0.05,\n",
    "                                                     frequency=252,\n",
    "                                                     compound_return=False,\n",
    "                                                     weight_bounds=(0,1),\n",
    "                                                     covarirance_method=\"sample_cov\",\n",
    "                                                     solver=\"SLSQP\",\n",
    "                                                     volatility_point=1000):\n",
    "    list_of_assets= df_return_data.columns.tolist()\n",
    "    list_of_simulated_portfolios=[]\n",
    "    #### Shake the retruns with some percentage of their standadrd deviation- Adding white noise with mean zero.\n",
    "    for shake in list_of_shaking_magnitude:\n",
    "        shake=shake/100\n",
    "        df_return_data_shaked = data_shaker(df_return_data=df_return_data,\n",
    "                                            shake_magnitude=shake,\n",
    "                                            multivariate_normal=multivariate_normal)\n",
    "        \n",
    "        df_weights= efficient_frontier_CVaR_calculator(df_return_data=df_return_data_shaked,\n",
    "                                                      CVaR_Significance=CVaR_Significance,\n",
    "                                                      frequency=frequency,\n",
    "                                                      compound_return=compound_return,\n",
    "                                                      weight_bounds=weight_bounds,\n",
    "                                                      covarirance_method=covarirance_method,\n",
    "                                                      solver=solver,\n",
    "                                                      portfolio_performance=False,\n",
    "                                                      volatility_point=volatility_point)\n",
    "        df_weights[\"shake\"]=shake\n",
    "        list_of_simulated_portfolios.append(df_weights)\n",
    "    ### Concatinate all the portfolios\n",
    "    df_simulated_efficient_frontier = pd.concat(list_of_simulated_portfolios,\n",
    "                                                ignore_index=True)\n",
    "    \n",
    "    ### Compute Portfolios performances\n",
    "    df_simulated_efficient_frontier = portfolio_performance_computer(df_return_data,\n",
    "                                                         df_simulated_efficient_frontier,\n",
    "                                                         list_of_assets,\n",
    "                                                         frequency)\n",
    "    return (df_simulated_efficient_frontier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Non-Resmapled Efficient Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_assets= df_stock_return_data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_weights_MVO=efficient_frontier_calculator(df_stock_return_data,\n",
    "                                  frequency=252,\n",
    "                                  covarirance_method=\"ledoit_wolf\",\n",
    "                                  compound_return=False,\n",
    "                                  portfolio_performance=True,\n",
    "                                  weight_bounds=(0,1),\n",
    "                                  volatility_point=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot Efficient Frontier\n",
    "plt.scatter(df_weights_MVO[\"Annualized Volatility\"],\n",
    "            df_weights_MVO[\"Annualized Simple Return\"])\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Simple Return\")\n",
    "plt.title(\"MVO Efficient Frontier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights_MVO.sort_values(\"Sharp Ratio\",\n",
    "                           ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot Asset Allocation\n",
    "df_weights_MVO.set_index(\"Annualized Volatility\")[list_of_all_assets].plot.area(figsize=(10,5))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Portfolio\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"MVO Portfolio Asset Allocations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Resmapled Efficient Frontier (multi Variate Normal)\n",
    "Consurtct Efficient Frontiter with simulated Return and Covriance matrix for 50 times and then take the average weights of each asset for earh risk averstion \n",
    "\n",
    "check out emperical multivariete distribution:\n",
    "1. https://machinelearningmastery.com/empirical-distribution-function-in-python/\n",
    "2. https://stats.stackexchange.com/questions/62146/method-for-generating-correlated-non-normal-data\n",
    "3. http://www.econometricsbysimulation.com/2014/02/easily-generate-correlated-variables.html\n",
    "4. https://www.newfrontieradvisors.com/media/1138/estimation-error-and-portfolio-optimization-12-05.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Simulate N number of efficient Frontier via Multivarite Normal Distribution\n",
    "df_weights_Multi_Normal_MVO_simulated = efficient_frontier_simulator_multivariate_normal(df_stock_return_data,\n",
    "                                                                                number_of_simulation=30,\n",
    "                                                                                frequency=252,\n",
    "                                                                                compound_return=False,\n",
    "                                                                                covarirance_method=\"ledoit_wolf\",\n",
    "                                                                                weight_bounds=(0, 1),\n",
    "                                                                                volatility_point=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resample the efficient frontier from N efficient frontier computed above\n",
    "df_weights_Multi_Normal_RMVO= efficient_frontier_resampler(df_weights_Multi_Normal_MVO_simulated,\n",
    "                                                           df_stock_return_data,\n",
    "                                                           volatility_spacing=0.02,\n",
    "                                                           frequency=252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Efficient Frontier\n",
    "df_weights_Multi_Normal_MVO_simulated.groupby(\"Simulation\").apply(lambda x: plt.plot(x[\"Annualized Volatility\"],\n",
    "                                                                                     x[\"Annualized Simple Return\"],\n",
    "                                                                                    color=\"lightblue\"))\n",
    "plt.plot(df_weights_Multi_Normal_RMVO[\"Annualized Volatility\"],\n",
    "         df_weights_Multi_Normal_RMVO[\"Annualized Simple Return\"],\n",
    "        color=\"red\")\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Simple Return\")\n",
    "plt.title(\"Resampled Efficient Frontier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plot Asset Allocation\n",
    "df_weights_Multi_Normal_RMVO.set_index(\"Annualized Volatility\")[list_of_all_assets].plot.area(figsize=(8.5,4.5))\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Portfolio Allocations via Resampling (Multi Variate Return) via Risk Aversion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Resmapled Efficient Frontier (Shaking)\n",
    "Consurtct Efficient Frontiter with shaking Return and Covriance matrix. \n",
    "Then take the average weights of portfolios in volatility buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_weights_shaked_MVO_simulated_MN1 = efficient_frontier_simulator_shaking_returns(df_stock_return_data,\n",
    "                                                                               list_of_shaking_magnitude=np.arange(25,35,.5),\n",
    "                                                                               multivariate_normal=True,\n",
    "                                                                               frequency=252,\n",
    "                                                                               compound_return=False,\n",
    "                                                                               covarirance_method=\"sample_cov\",\n",
    "                                                                               volatility_point=1800,\n",
    "                                                                               weight_bounds=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights_shaked_MVO_simulated_MN1_resampled=efficient_frontier_resampler(df_weights_shaked_MVO_simulated_MN1,\n",
    "                                                     df_stock_return_data,\n",
    "                                                     volatility_spacing=0.02,\n",
    "                                                     frequency=252)\n",
    "\n",
    "df_weights_shaked_MVO_simulated_MN1.groupby(['shake']).apply(lambda x: plt.plot(x[\"Annualized Volatility\"],\n",
    "                                                                                x[\"Annualized Simple Return\"],\n",
    "                                                                                color=\"lightblue\"))\n",
    "\n",
    "plt.plot(df_weights_shaked_MVO_simulated_MN1_resampled[\"Annualized Volatility\"],\n",
    "         df_weights_shaked_MVO_simulated_MN1_resampled[\"Annualized Simple Return\"],\n",
    "         color='red')\n",
    "plt.show()\n",
    "\n",
    "### Plot Asset Allocation\n",
    "df_weights_shaked_MVO_simulated_MN1_resampled.set_index(\"Annualized Volatility\")[list_of_all_assets].plot.area(figsize=(10,5))\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Portfolio Allocations via Resampling - Shaking Returns Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_portfolio_CVaR_calculator(df_stock_return_data,\n",
    "                                        CVaR_Significance=0.05,\n",
    "                                        target_risk=None,\n",
    "                                        frequency=252,\n",
    "                                        compound_return=False,\n",
    "                                        weight_bounds=(0,1),\n",
    "                                        covarirance_method=\"ledoit_wolf\",\n",
    "                                        solver=\"SLSQP\",\n",
    "                                        portfolio_performance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_weights_cvar_MVO = efficient_frontier_CVaR_calculator(df_stock_return_data,\n",
    "                                                         CVaR_Significance=0.05,\n",
    "                                                         frequency=252,\n",
    "                                                         compound_return=False,\n",
    "                                                         weight_bounds=(0,1),\n",
    "                                                         covarirance_method=\"sample_cov\",\n",
    "                                                         solver=\"SLSQP\",\n",
    "                                                         portfolio_performance=True,\n",
    "                                                         volatility_point=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Efficient Frontier\n",
    "plt.plot(df_weights_cvar_MVO[\"Annualized Volatility\"],\n",
    "         df_weights_cvar_MVO[\"Annualized Simple Return\"])\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Simple Return\")\n",
    "plt.title(\"CVaR Efficient Portfolios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights_cvar_MVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "efficient_frontier_resampler(df_weights_cvar_MVO,\n",
    "                             df_stock_return_data,\n",
    "                             volatility_spacing=0.004,\n",
    "                             frequency=252)[list_of_all_assets].plot.area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot CVaR Asset Allocation\n",
    "df_weights_cvar_MVO[list_of_all_assets].plot.area(figsize=(7,5))\n",
    "plt.xlabel(\"Portfolio\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"CVaR Asset Allocation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resmapled CVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_weights_shaked_cvar_mvo = efficient_frontier_CVaR_simulator_shaking_returns(df_stock_return_data,\n",
    "                                                     list_of_shaking_magnitude=np.arange(25,35),\n",
    "                                                      multivariate_normal=True,\n",
    "                                                     CVaR_Significance=0.05,\n",
    "                                                     frequency=252,\n",
    "                                                     compound_return=False,\n",
    "                                                     weight_bounds=(0,1),\n",
    "                                                     covarirance_method=\"sample_cov\",\n",
    "                                                     solver=\"SLSQP\",\n",
    "                                                     volatility_point=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resample the efficient frontier from N efficient frontier computed above\n",
    "df_weights_cvar_resampled_mvo= efficient_frontier_resampler(df_weights_shaked_cvar_mvo,\n",
    "                                                            df_stock_return_data,\n",
    "                                                            volatility_spacing=0.005,\n",
    "                                                            frequency=252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights_shaked_cvar_mvo.groupby(\"shake\").apply(lambda x: plt.plot(x[\"Annualized Volatility\"],\n",
    "                                                                     x[\"Annualized Simple Return\"],\n",
    "                                                                    color=\"lightblue\"))\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Simple Return\")\n",
    "### Plot Efficient Frontier\n",
    "plt.plot(df_weights_cvar_resampled_mvo[\"Annualized Volatility\"],\n",
    "            df_weights_cvar_resampled_mvo[\"Annualized Simple Return\"],color=\"red\")\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Simple Return\")\n",
    "plt.title(\"Rasampled CVaR Efficient Portfolios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot CVaR Asset Allocation\n",
    "df_weights_cvar_resampled_mvo.set_index(\"Annualized Volatility\")[df_stock_return_data.columns].plot.area(figsize=(10,4.5))\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Resampled Portfolio Allocation (CVaR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
